summary<-data.frame(
file=c("blogs","news","twitter"),
file_size_mb=size_mb,
num_lines=lines,
num_words=c(blogs_wpf,news_wpf,twitter_wpf),
words_per_line=c(blogs_wpl,news_wpl,twitter_wpl)
)
kable(summary) %>%
kable_styling("striped", full_width = F)
#twit_love_Index <- grepl(".*love.*", twitter)
#length(twit_love_Index)
#twit_hate_Index <- grepl(".*hate.*", twitter)
#sum(twit_hate_Index)
#sum(twit_love_Index)/sum(twit_hate_Index)
#twitter[[grep(".biostats.",twitter)]]
#grep("A computer once beat me at chess, but it was no match for me at kickboxing", twitter)
set.seed(1234)
#Samples
blog_sample<-sample(blogs,lines[1]*0.1,replace=F)
news_sample<-sample(news,lines[1]*0.1,replace=F)
twitter_sample<-sample(twitter,lines[1]*0.1,replace=F)
sample10<-c(blog_sample,news_sample,twitter_sample)
#sample lengths
length(sample10);sum(stri_count_words(sample10))
##write sample
writeLines(sample10,
"./sample10.txt")
#remove objects
rm(blogs);rm(news);rm(twitter)
rm(blog_sample);rm(news_sample);rm(twitter_sample)
fileURL <- "https://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words_comma-separated-text-file_2018_07_30.zip"
download.file(fileURL,destfile="full-list-of-bad-words_comma-separated-text-file_2018_07_30.zip")
unzip(zipfile="full-list-of-bad-words_comma-separated-text-file_2018_07_30.zip")
## load the expletives
badCon <- file("full-list-of-bad-words_comma-separated-text-file_2018_07_30.txt", open = "rb")
badwords <- suppressWarnings(readLines(badCon, encoding = "UTF-8", skipNul=TRUE))
close(badCon)
##removing non-ascii characters
badwords<-iconv(badwords,"latin1","ASCII",sub="")
sample10 <-iconv(sample10,"latin1","ASCII",sub="")
##remove profanities
sample10 <- removeWords(sample10,badwords)
#Tips from https://stackoverflow.com/questions/9637278/r-tm-package-invalid-input-in-utf8towcs
corpus <- Corpus(VectorSource(sample10))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
#Remove strange characters
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern," ",x))})
corpus<- tm_map(corpus,toSpace,"[^[:graph:]]")
#Remove URL
corpus<- tm_map(corpus,toSpace,"(http[^ ]*)|(ftp[^ ]*)|(www\\.[^ ]*)")
#Remove email address
corpus<- tm_map(corpus,toSpace,"([_+a-z0-9-]+(\\.[_+a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,14}))")
#Remove Twitter pseudo (ex: @bidule)
corpus<- tm_map(corpus,toSpace,"@[_+a-z0-9-]{3,16}")
#Remove Twitter RT things
corpus<- tm_map(corpus,toSpace,"^RT")
#Remove Emoticon
corpus<- tm_map(corpus,toSpace,">?[:;=8XB]{1}[-~+o^]?[|\")(&gt;DO>{pP3/]+|</?3|XD+|D:<|x[-~+o^]?[|\")(&gt;DO>{pP3/]+")
#Remove # from hashtag
corpus<- tm_map(corpus,toSpace,"#")
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
#Transform to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, stripWhitespace) #remove Multiple whitespace characters
corpus <- tm_map(corpus, stripWhitespace) #remove Multiple whitespace characters again
#Save the cleaned corpus
#writeCorpus(corpus, path = "./corpus/",
#            filenames = paste(seq_along(corpus), ".txt", sep = ""))
#Test after cleaning : read the 2017th tweet again
corpus[[3]]$content[2017]
sampleClean<-get("content",corpus)
save("sampleClean",file="sampleClean.rData")
rm(sample10);rm(corpus)
library(wordcloud)
suppressWarnings(wordcloud(sampleClean,max.words=500,random.order=FALSE, colors=brewer.pal(9, "RdYlGn")))
suppressMessages(library(quanteda))
tokenizedDF <- function(obj, n) {
nGramSparse <- dfm(obj, ngrams= n, concatenator = " ")
nGramDF <- data.frame(Content = featnames(nGramSparse), Frequency = colSums(nGramSparse),
row.names = NULL, stringsAsFactors = FALSE)
}
library(ggplot2)
top10Plot <- function(df, title) {
ggplot(df[1:10,], aes(reorder(Content, Frequency), Frequency)) +
labs(x = "Word(s)", y = "Frequency") +
theme(axis.text.x = element_text(angle = 90, size = 10, hjust = 1)) +
coord_flip() +
ggtitle(title) +
geom_bar(stat = "identity", fill = I("purple1"))
}
# uni-gram
uniGram <- tokenizedDF(sampleClean, 1)
# sorted
uniGram <- uniGram[order(uniGram$Frequency,decreasing = TRUE),]
#head(uniGram,10)
uniGramData<-get("content",uniGram)
view(final2Data)
head(final2Data)
head(final2Data.data)
head(final2Data.Rdata)
head(unigram)
?get
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(ngram)
library(data.table)
library(sqldf)
library(quanteda)
library(stringi)
library(knitr)
library(kableExtra)
blogs<-readLines("final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
twitter<-readLines("final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
newsCon <- file("./final/en_US/en_US.news.txt", open = "rb")
news<-readLines(newsCon, encoding = "UTF-8", skipNul = TRUE)
close(newsCon)
##File Size
size_mb<-round(file.info(c("./final/en_US/en_US.blogs.txt","./final/en_US/en_US.news.txt","./final/en_US/en_US.twitter.txt"))$size/1024^2)
##Number of lines
lines<-sapply(list(blogs,news,twitter),length)
###Words per file
blogs_wpf<-sum(stri_count_words(blogs))
news_wpf<-sum(stri_count_words(news))
twitter_wpf<-sum(stri_count_words(twitter))
###Words per line
blogs_wpl<-mean(stri_count_words(blogs))
news_wpl<-mean(stri_count_words(news))
twitter_wpl<-mean(stri_count_words(twitter))
##Summary
summary<-data.frame(
file=c("blogs","news","twitter"),
file_size_mb=size_mb,
num_lines=lines,
num_words=c(blogs_wpf,news_wpf,twitter_wpf),
words_per_line=c(blogs_wpl,news_wpl,twitter_wpl)
)
kable(summary) %>%
kable_styling("striped", full_width = F)
#twit_love_Index <- grepl(".*love.*", twitter)
#length(twit_love_Index)
#twit_hate_Index <- grepl(".*hate.*", twitter)
#sum(twit_hate_Index)
#sum(twit_love_Index)/sum(twit_hate_Index)
#twitter[[grep(".biostats.",twitter)]]
#grep("A computer once beat me at chess, but it was no match for me at kickboxing", twitter)
set.seed(1234)
#Samples
blog_sample<-sample(blogs,lines[1]*0.1,replace=F)
news_sample<-sample(news,lines[1]*0.1,replace=F)
twitter_sample<-sample(twitter,lines[1]*0.1,replace=F)
sample10<-c(blog_sample,news_sample,twitter_sample)
#sample lengths
length(sample10);sum(stri_count_words(sample10))
##write sample
writeLines(sample10,
"./sample10.txt")
#remove objects
rm(blogs);rm(news);rm(twitter)
rm(blog_sample);rm(news_sample);rm(twitter_sample)
fileURL <- "https://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words_comma-separated-text-file_2018_07_30.zip"
download.file(fileURL,destfile="full-list-of-bad-words_comma-separated-text-file_2018_07_30.zip")
unzip(zipfile="full-list-of-bad-words_comma-separated-text-file_2018_07_30.zip")
## load the expletives
badCon <- file("full-list-of-bad-words_comma-separated-text-file_2018_07_30.txt", open = "rb")
badwords <- suppressWarnings(readLines(badCon, encoding = "UTF-8", skipNul=TRUE))
close(badCon)
##removing non-ascii characters
badwords<-iconv(badwords,"latin1","ASCII",sub="")
sample10 <-iconv(sample10,"latin1","ASCII",sub="")
##remove profanities
sample10 <- removeWords(sample10,badwords)
#Tips from https://stackoverflow.com/questions/9637278/r-tm-package-invalid-input-in-utf8towcs
corpus <- Corpus(VectorSource(sample10))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
#Remove strange characters
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern," ",x))})
corpus<- tm_map(corpus,toSpace,"[^[:graph:]]")
#Remove URL
corpus<- tm_map(corpus,toSpace,"(http[^ ]*)|(ftp[^ ]*)|(www\\.[^ ]*)")
#Remove email address
corpus<- tm_map(corpus,toSpace,"([_+a-z0-9-]+(\\.[_+a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,14}))")
#Remove Twitter pseudo (ex: @bidule)
corpus<- tm_map(corpus,toSpace,"@[_+a-z0-9-]{3,16}")
#Remove Twitter RT things
corpus<- tm_map(corpus,toSpace,"^RT")
#Remove Emoticon
corpus<- tm_map(corpus,toSpace,">?[:;=8XB]{1}[-~+o^]?[|\")(&gt;DO>{pP3/]+|</?3|XD+|D:<|x[-~+o^]?[|\")(&gt;DO>{pP3/]+")
#Remove # from hashtag
corpus<- tm_map(corpus,toSpace,"#")
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
#Transform to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, stripWhitespace) #remove Multiple whitespace characters
corpus <- tm_map(corpus, stripWhitespace) #remove Multiple whitespace characters again
#Save the cleaned corpus
#writeCorpus(corpus, path = "./corpus/",
#            filenames = paste(seq_along(corpus), ".txt", sep = ""))
#Test after cleaning : read the 2017th tweet again
corpus[[3]]$content[2017]
sampleClean<-get("content",corpus)
save("sampleClean",file="sampleClean.rData")
mydfm<-data.frame(text=)
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(ngram)
library(data.table)
library(sqldf)
library(quanteda)
library(stringi)
library(knitr)
library(kableExtra)
blogs<-readLines("final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
twitter<-readLines("final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
newsCon <- file("./final/en_US/en_US.news.txt", open = "rb")
news<-readLines(newsCon, encoding = "UTF-8", skipNul = TRUE)
close(newsCon)
##File Size
size_mb<-round(file.info(c("./final/en_US/en_US.blogs.txt","./final/en_US/en_US.news.txt","./final/en_US/en_US.twitter.txt"))$size/1024^2)
##Number of lines
lines<-sapply(list(blogs,news,twitter),length)
###Words per file
blogs_wpf<-sum(stri_count_words(blogs))
news_wpf<-sum(stri_count_words(news))
twitter_wpf<-sum(stri_count_words(twitter))
###Words per line
blogs_wpl<-mean(stri_count_words(blogs))
news_wpl<-mean(stri_count_words(news))
twitter_wpl<-mean(stri_count_words(twitter))
##Summary
summary<-data.frame(
file=c("blogs","news","twitter"),
file_size_mb=size_mb,
num_lines=lines,
num_words=c(blogs_wpf,news_wpf,twitter_wpf),
words_per_line=c(blogs_wpl,news_wpl,twitter_wpl)
)
kable(summary) %>%
kable_styling("striped", full_width = F)
#twit_love_Index <- grepl(".*love.*", twitter)
#length(twit_love_Index)
#twit_hate_Index <- grepl(".*hate.*", twitter)
#sum(twit_hate_Index)
#sum(twit_love_Index)/sum(twit_hate_Index)
#twitter[[grep(".biostats.",twitter)]]
#grep("A computer once beat me at chess, but it was no match for me at kickboxing", twitter)
set.seed(1234)
#Samples
blog_sample<-sample(blogs,lines[1]*0.1,replace=F)
news_sample<-sample(news,lines[1]*0.1,replace=F)
twitter_sample<-sample(twitter,lines[1]*0.1,replace=F)
sample10<-c(blog_sample,news_sample,twitter_sample)
#sample lengths
length(sample10);sum(stri_count_words(sample10))
##write sample
writeLines(sample10,
"./sample10.txt")
#remove objects
rm(blogs);rm(news);rm(twitter)
rm(blog_sample);rm(news_sample);rm(twitter_sample)
fileURL <- "https://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words_comma-separated-text-file_2018_07_30.zip"
download.file(fileURL,destfile="full-list-of-bad-words_comma-separated-text-file_2018_07_30.zip")
unzip(zipfile="full-list-of-bad-words_comma-separated-text-file_2018_07_30.zip")
## load the expletives
badCon <- file("full-list-of-bad-words_comma-separated-text-file_2018_07_30.txt", open = "rb")
badwords <- suppressWarnings(readLines(badCon, encoding = "UTF-8", skipNul=TRUE))
close(badCon)
##removing non-ascii characters
badwords<-iconv(badwords,"latin1","ASCII",sub="")
sample10 <-iconv(sample10,"latin1","ASCII",sub="")
##remove profanities
sample10 <- removeWords(sample10,badwords)
#Tips from https://stackoverflow.com/questions/9637278/r-tm-package-invalid-input-in-utf8towcs
corpus <- Corpus(VectorSource(sample10))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
#Remove strange characters
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern," ",x))})
corpus<- tm_map(corpus,toSpace,"[^[:graph:]]")
#Remove URL
corpus<- tm_map(corpus,toSpace,"(http[^ ]*)|(ftp[^ ]*)|(www\\.[^ ]*)")
#Remove email address
corpus<- tm_map(corpus,toSpace,"([_+a-z0-9-]+(\\.[_+a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,14}))")
#Remove Twitter pseudo (ex: @bidule)
corpus<- tm_map(corpus,toSpace,"@[_+a-z0-9-]{3,16}")
#Remove Twitter RT things
corpus<- tm_map(corpus,toSpace,"^RT")
#Remove Emoticon
corpus<- tm_map(corpus,toSpace,">?[:;=8XB]{1}[-~+o^]?[|\")(&gt;DO>{pP3/]+|</?3|XD+|D:<|x[-~+o^]?[|\")(&gt;DO>{pP3/]+")
#Remove # from hashtag
corpus<- tm_map(corpus,toSpace,"#")
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
#Transform to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, stripWhitespace) #remove Multiple whitespace characters
corpus <- tm_map(corpus, stripWhitespace) #remove Multiple whitespace characters again
#Save the cleaned corpus
#writeCorpus(corpus, path = "./corpus/",
#            filenames = paste(seq_along(corpus), ".txt", sep = ""))
#Test after cleaning : read the 2017th tweet again
corpus[[3]]$content[2017]
sampleClean<-get("content",corpus)
save("sampleClean",file="sampleClean.rData")
rm(sample10);rm(corpus)
library(wordcloud)
suppressWarnings(wordcloud(sampleClean,max.words=500,random.order=FALSE, colors=brewer.pal(9, "RdYlGn")))
suppressMessages(library(quanteda))
tokenizedDF <- function(obj, n) {
nGramSparse <- dfm(obj, ngrams= n, concatenator = " ")
nGramDF <- data.frame(Content = featnames(nGramSparse), Frequency = colSums(nGramSparse),
row.names = NULL, stringsAsFactors = FALSE)
}
library(ggplot2)
top10Plot <- function(df, title) {
ggplot(df[1:10,], aes(reorder(Content, Frequency), Frequency)) +
labs(x = "Word(s)", y = "Frequency") +
theme(axis.text.x = element_text(angle = 90, size = 10, hjust = 1)) +
coord_flip() +
ggtitle(title) +
geom_bar(stat = "identity", fill = I("purple1"))
}
# uni-gram
uniGram <- tokenizedDF(sampleClean, 1)
# sorted
uniGram <- uniGram[order(uniGram$Frequency,decreasing = TRUE),]
#head(uniGram,10)
uniGramData<-get(uniGram)
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(ngram)
library(data.table)
library(sqldf)
library(quanteda)
library(stringi)
library(knitr)
library(kableExtra)
blogs<-readLines("final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
twitter<-readLines("final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
newsCon <- file("./final/en_US/en_US.news.txt", open = "rb")
news<-readLines(newsCon, encoding = "UTF-8", skipNul = TRUE)
close(newsCon)
##File Size
size_mb<-round(file.info(c("./final/en_US/en_US.blogs.txt","./final/en_US/en_US.news.txt","./final/en_US/en_US.twitter.txt"))$size/1024^2)
##Number of lines
lines<-sapply(list(blogs,news,twitter),length)
###Words per file
blogs_wpf<-sum(stri_count_words(blogs))
news_wpf<-sum(stri_count_words(news))
twitter_wpf<-sum(stri_count_words(twitter))
###Words per line
blogs_wpl<-mean(stri_count_words(blogs))
news_wpl<-mean(stri_count_words(news))
twitter_wpl<-mean(stri_count_words(twitter))
##Summary
summary<-data.frame(
file=c("blogs","news","twitter"),
file_size_mb=size_mb,
num_lines=lines,
num_words=c(blogs_wpf,news_wpf,twitter_wpf),
words_per_line=c(blogs_wpl,news_wpl,twitter_wpl)
)
kable(summary) %>%
kable_styling("striped", full_width = F)
#twit_love_Index <- grepl(".*love.*", twitter)
#length(twit_love_Index)
#twit_hate_Index <- grepl(".*hate.*", twitter)
#sum(twit_hate_Index)
#sum(twit_love_Index)/sum(twit_hate_Index)
#twitter[[grep(".biostats.",twitter)]]
#grep("A computer once beat me at chess, but it was no match for me at kickboxing", twitter)
set.seed(1234)
#Samples
blog_sample<-sample(blogs,lines[1]*0.1,replace=F)
news_sample<-sample(news,lines[1]*0.1,replace=F)
twitter_sample<-sample(twitter,lines[1]*0.1,replace=F)
sample10<-c(blog_sample,news_sample,twitter_sample)
#sample lengths
length(sample10);sum(stri_count_words(sample10))
##write sample
writeLines(sample10,
"./sample10.txt")
#remove objects
rm(blogs);rm(news);rm(twitter)
rm(blog_sample);rm(news_sample);rm(twitter_sample)
fileURL <- "https://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words_comma-separated-text-file_2018_07_30.zip"
download.file(fileURL,destfile="full-list-of-bad-words_comma-separated-text-file_2018_07_30.zip")
unzip(zipfile="full-list-of-bad-words_comma-separated-text-file_2018_07_30.zip")
## load the expletives
badCon <- file("full-list-of-bad-words_comma-separated-text-file_2018_07_30.txt", open = "rb")
badwords <- suppressWarnings(readLines(badCon, encoding = "UTF-8", skipNul=TRUE))
close(badCon)
##removing non-ascii characters
badwords<-iconv(badwords,"latin1","ASCII",sub="")
sample10 <-iconv(sample10,"latin1","ASCII",sub="")
##remove profanities
sample10 <- removeWords(sample10,badwords)
#Tips from https://stackoverflow.com/questions/9637278/r-tm-package-invalid-input-in-utf8towcs
corpus <- Corpus(VectorSource(sample10))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
#Remove strange characters
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern," ",x))})
corpus<- tm_map(corpus,toSpace,"[^[:graph:]]")
#Remove URL
corpus<- tm_map(corpus,toSpace,"(http[^ ]*)|(ftp[^ ]*)|(www\\.[^ ]*)")
#Remove email address
corpus<- tm_map(corpus,toSpace,"([_+a-z0-9-]+(\\.[_+a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,14}))")
#Remove Twitter pseudo (ex: @bidule)
corpus<- tm_map(corpus,toSpace,"@[_+a-z0-9-]{3,16}")
#Remove Twitter RT things
corpus<- tm_map(corpus,toSpace,"^RT")
#Remove Emoticon
corpus<- tm_map(corpus,toSpace,">?[:;=8XB]{1}[-~+o^]?[|\")(&gt;DO>{pP3/]+|</?3|XD+|D:<|x[-~+o^]?[|\")(&gt;DO>{pP3/]+")
#Remove # from hashtag
corpus<- tm_map(corpus,toSpace,"#")
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
#Transform to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, stripWhitespace) #remove Multiple whitespace characters
corpus <- tm_map(corpus, stripWhitespace) #remove Multiple whitespace characters again
#Save the cleaned corpus
#writeCorpus(corpus, path = "./corpus/",
#            filenames = paste(seq_along(corpus), ".txt", sep = ""))
#Test after cleaning : read the 2017th tweet again
corpus[[3]]$content[2017]
sampleClean<-get("content",corpus)
save("sampleClean",file="sampleClean.rData")
rm(sample10);rm(corpus)
library(wordcloud)
suppressWarnings(wordcloud(sampleClean,max.words=500,random.order=FALSE, colors=brewer.pal(9, "RdYlGn")))
suppressMessages(library(quanteda))
tokenizedDF <- function(obj, n) {
nGramSparse <- dfm(obj, ngrams= n, concatenator = " ")
nGramDF <- data.frame(Content = featnames(nGramSparse), Frequency = colSums(nGramSparse),
row.names = NULL, stringsAsFactors = FALSE)
}
library(ggplot2)
top10Plot <- function(df, title) {
ggplot(df[1:10,], aes(reorder(Content, Frequency), Frequency)) +
labs(x = "Word(s)", y = "Frequency") +
theme(axis.text.x = element_text(angle = 90, size = 10, hjust = 1)) +
coord_flip() +
ggtitle(title) +
geom_bar(stat = "identity", fill = I("purple1"))
}
# uni-gram
uniGram <- tokenizedDF(sampleClean, 1)
# sorted
uniGram <- uniGram[order(uniGram$Frequency,decreasing = TRUE),]
#head(uniGram,10)
save("uniGram",file="uniGramData.rData")
#plot top 10
top10Plot(uniGram, "Top 10 unigrams")
biGram <- tokenizedDF(sampleClean, 2)
#sorted
biGram <- biGram[order(biGram$Frequency,decreasing = TRUE),]
save("biGram",file="biGramData.rData")
#plot top 10
top10Plot(biGram, "Top 10 bigrams")
triGram <- tokenizedDF(sampleClean, 3)
#sorted
triGram <- triGram[order(triGram$Frequency,decreasing = TRUE),]
save("triGram",file="triGramData.rData")
#plot top 10
top10Plot(triGram, "Top 10 trigrams")
quadGram <- tokenizedDF(sampleClean, 4)
#sorted
quadGram <- quadGram[order(quadGram$Frequency,decreasing = TRUE),]
save("quadGram",file="quadGramData.rData")
#plot top 10
top10Plot(quadGram, "Top 10 quadgrams")
head(quadGram)
str(quadGram)
summary(quadGram    )
shiny::runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
install.packages("shinythemes")
runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
shiny::runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
/c
\c
\q
exit
/q
/c
install.packages("stylo
")
runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
library(stylo)
shiny::runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
install.packages("shyno")
shiny::runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
install.packages("stylo")
runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
shiny::runApp('D:/Programs/R/Github/Shiny/CapstoneApp')
rsconnect::setAccountInfo(name='magnusrex86', token='BA09A8FC38BBC026849E14B4D2474881', secret='hH0MlufvogA5wrSWT8DcZUahJVroSOia+63DNduM')
setwd("D:/Programs/R/Github/Shiny/CapstoneApp")
runApp()
runApp()
library(lobstr)
install.packages("lobstr")
mem_used()
